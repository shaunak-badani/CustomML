{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e7eb865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e47c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(flatten=False):\n",
    "    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    # normalize x\n",
    "    X_train = X_train.astype(float) / 255.\n",
    "    X_test = X_test.astype(float) / 255.\n",
    "    # we reserve the last 10000 training examples for validation\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "    if flatten:\n",
    "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
    "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
    "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3596752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, Xn_val, y_val, X_test, y_test = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c076dbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28) (50000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe1a8517a10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Printing dimensions\n",
    "print(X_train.shape, y_train.shape)\n",
    "## Visualizing the first digit\n",
    "plt.imshow(X_train[2], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99a5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5f674e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51655f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a8ad3",
   "metadata": {},
   "source": [
    "### Making the one hot encoded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "119137aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_t = np.zeros((y_train.size, 10))\n",
    "y_t[np.arange(y_train.size), y_train] = 1\n",
    "print(y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31896299",
   "metadata": {},
   "source": [
    "### Making a deterministic MLP with the following parameters : \n",
    "* no dropout layer\n",
    "* constant learning rate = 0.001\n",
    "* 784 -> 512 -> 256 -> 10\n",
    "* input -> relu -> relu -> relu -> softmax -> sig\n",
    "* no regularization\n",
    "* loss = mean error of (y - y_hat), not MSE\n",
    "* Gradient Descent Optimizer is being used\n",
    "* Taking a particular batch for one epoch, to deterministically evaluate values and gradients of network variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93989734",
   "metadata": {},
   "source": [
    "### Making a custom MLP with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bfbc5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4b4ae83a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-ddaaa9c7154e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-105-ddaaa9c7154e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, batch_size, epochs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m#                 print(self.loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mind\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mind\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {}: Loss {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-ddaaa9c7154e>\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class CustomMLP:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.w1 = np.random.normal(size = (512, 784)) * np.sqrt(1 / 784)\n",
    "        self.b1 = np.random.normal(size = (512, 1))\n",
    "        self.w2 = np.random.normal(size = (256, 512))* np.sqrt(1 / 512)\n",
    "        self.b2 = np.random.normal(size = (256, 1))\n",
    "        self.w3 = np.random.normal(size = (10, 256)) * np.sqrt(1 / 256)\n",
    "        self.b3 = np.random.normal(size = (10, 1))\n",
    "        self.learning_rate = 0.01\n",
    "    \n",
    "    def relu(self, a):\n",
    "        a[a < 0] = 0\n",
    "        return a\n",
    "    \n",
    "    def relu_der(self, a):\n",
    "        j = np.empty_like(a)\n",
    "        j[a < 0] = 0\n",
    "        j[a > 0] = 1\n",
    "        \n",
    "        return j\n",
    "    \n",
    "    \n",
    "    def sigmoid(self, a):\n",
    "        j = np.empty_like(a)\n",
    "        j[a < 0] = np.exp(a[a < 0]) / (1 + np.exp(a[a < 0]))\n",
    "        j[a > 0] = 1 / (1 + np.exp(-a[a > 0]))\n",
    "        \n",
    "        return j\n",
    "    \n",
    "    def forward_prop(self, X, Y):\n",
    "        x = X.T\n",
    "        y = Y.T\n",
    "        \n",
    "                \n",
    "        self.z1 = self.w1 @ x + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        \n",
    "        self.z2 = self.w2 @ self.a1 + self.b2\n",
    "        self.a2 = self.relu(self.z2)\n",
    "        \n",
    "        self.z3 = self.w3 @ self.a2 + self.b3\n",
    "        self.a3 = self.sigmoid(self.z3)\n",
    "        \n",
    "        \n",
    "        # softmax\n",
    "#         p = np.exp(self.a3 - self.a3.max(axis = 0, keepdims = True))\n",
    "        p = np.exp(self.a3)\n",
    "#         print(p.shape)\n",
    "        self.p = p / p.sum(axis = 0, keepdims = True)\n",
    "#         print(np.log(self.p))\n",
    "        # Cross entropy loss\n",
    "#         print(np.log(self.p))\n",
    "        self.loss = np.mean(-y * np.log(self.p))\n",
    "    \n",
    "    def backward_prop(self, X, Y):\n",
    "        \n",
    "        x = X.T\n",
    "        y = Y.T\n",
    "        \n",
    "        # Backward propagation\n",
    "        dJ_dz3 = (self.p - y) * self.a3 * (1 - self.a3)\n",
    "        dJ_dw3 = dJ_dz3 @ self.a2.T\n",
    "        dJ_db3 = dJ_dz3\n",
    "        \n",
    "        dJ_dz2 = (self.w3.T @ dJ_dz3) * self.relu_der(self.z2)\n",
    "        dJ_dw2 = dJ_dz2 @ self.a1.T\n",
    "        dJ_db2 = dJ_dz2\n",
    "        \n",
    "        dJ_dz1 = (self.w2.T @ dJ_dz2) * self.relu_der(self.z1)\n",
    "        dJ_dw1 = dJ_dz1 @ x.T\n",
    "        dJ_db1 = dJ_dz1\n",
    "        \n",
    "        # Backpropagation step\n",
    "        self.w3 -= self.learning_rate * np.mean(dJ_dw3, axis = 1, keepdims = True)\n",
    "        self.b3 -= self.learning_rate * np.mean(dJ_db3, axis = 1, keepdims = True)\n",
    "        \n",
    "        self.w2 -= self.learning_rate * np.mean(dJ_dw2, axis = 1, keepdims = True)\n",
    "        self.b2 -= self.learning_rate * np.mean(dJ_db2, axis = 1, keepdims = True)\n",
    "        \n",
    "        self.w1 -= self.learning_rate * np.mean(dJ_dw1, axis = 1, keepdims = True)\n",
    "        self.b1 -= self.learning_rate * np.mean(dJ_db1, axis = 1, keepdims = True)\n",
    "        \n",
    "    \n",
    "    def train(self, X, y, batch_size = 32, epochs = 10):\n",
    "        N = X.shape[0]\n",
    "        losses = []\n",
    "        loss = 0\n",
    "        for epoch in range(epochs):\n",
    "            for ind in range(0, N, batch_size):\n",
    "                self.forward_prop(X[ind:ind + batch_size], y[ind:ind + batch_size])\n",
    "#                 print(self.loss)\n",
    "                self.backward_prop(X[ind:ind + batch_size], y[ind:ind + batch_size])\n",
    "            self.forward_prop(X, y)\n",
    "            \n",
    "            print(\"Epoch {}: Loss {}\".format(epoch, self.loss))\n",
    "            losses.append(self.loss)\n",
    "        \n",
    "k = CustomMLP()\n",
    "    \n",
    "k.train(X_train, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6a1a1f",
   "metadata": {},
   "source": [
    "## Using pytorch to compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2f93dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# define NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        # number of hidden nodes in each layer (512)\n",
    "        hidden_1 = 512\n",
    "        hidden_2 = 256\n",
    "        num_classes = 10\n",
    "        # linear layer (784 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(28*28, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1,hidden_2)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(hidden_2, num_classes)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self,x, y):\n",
    "#         x = x.view(1, -1)\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "#         p = torch.softmax(x, axis = 0)\n",
    "        print(y)\n",
    "        a = torch.LongTensor(y)\n",
    "        print(a.shape, \" a shape\")\n",
    "        print(x.shape, \" x shape\")\n",
    "#         print(p.shape, y.shape)\n",
    "        loss = self.loss(x, a)\n",
    "        loss.backward()\n",
    "#         cross_entropy_loss =\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        for i in self.parameters():\n",
    "            print(type(i))\n",
    "            print(i.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93288dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "90b8ff8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2433341903143396"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0, 0, 1, 0])\n",
    "k = np.exp(a)\n",
    "k /= k.sum()\n",
    "(-np.log(k[3]) + 0.743)/ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "212473cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4]) torch.Size([2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.2437)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "output = torch.autograd.Variable(torch.FloatTensor([[0, 0, 0, 1], [0, 0, 1, 0]])).view(2, -1)\n",
    "target = torch.autograd.Variable(torch.LongTensor([3, 3]))\n",
    "loss = nn.CrossEntropyLoss()\n",
    "print(output.shape, target.shape)\n",
    "loss(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5240e0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 784])\n",
      "[5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9 4 0 9 1 1 2 4 3 2 7 3 8 6 9 0 5 6\n",
      " 0 7 6 1 8 7 9 3 9 8 5 9 3 3 0 7 4 9 8 0 9 4 1 4 4 6 0 4 5 6 1 0 0 1 7 1 6\n",
      " 3 0 2 1 1 7 9 0 2 6 7 8 3 9 0 4 6 7 4 6 8 0 7 8 3 1 5 7 1 7 1 1 6 3 0 2 9\n",
      " 3 1 1 0 4 9 2 0 0 2 0 2 7 1 8 6 4]\n",
      "torch.Size([128])  a shape\n",
      "torch.Size([128, 10])  x shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.3022, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "import torch\n",
    "a = torch.tensor(X_train).type(torch.FloatTensor)\n",
    "i = 0\n",
    "# print(a[i].shape)\n",
    "# print(y_train.shape)\n",
    "batch = 128\n",
    "net.forward(a[:batch], y_train[:batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dc1f827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "torch.Size([512, 784])\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "torch.Size([512])\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "torch.Size([256, 512])\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "torch.Size([256])\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "torch.Size([10, 256])\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "net.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba8b76",
   "metadata": {},
   "source": [
    "### Computing own gradients using the same parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "18630675",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = []\n",
    "for i in net.parameters():\n",
    "    parameters.append(i.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2006be04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 784)\n"
     ]
    }
   ],
   "source": [
    "print(parameters[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f2c72f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedMLP(CustomMLP):\n",
    "    \n",
    "    def __init__(self):\n",
    "        parameters = []\n",
    "        for i in net.parameters():\n",
    "            parameters.append(i.detach().numpy())\n",
    "        self.w1 = parameters[0]\n",
    "        self.b1 = parameters[1].reshape(-1, 1)\n",
    "        self.w2 = parameters[2]\n",
    "        self.b2 = parameters[3].reshape(-1, 1)\n",
    "        self.w3 = parameters[4]\n",
    "        self.b3 = parameters[5].reshape(-1, 1)\n",
    "        self.learning_rate = 0.01\n",
    "    \n",
    "    def relu(self, a):\n",
    "        a[a < 0] = 0\n",
    "        return a\n",
    "    \n",
    "    def relu_der(self, a):\n",
    "        j = np.empty_like(a)\n",
    "        j[a < 0] = 0\n",
    "        j[a > 0] = 1\n",
    "        \n",
    "        return j\n",
    "    \n",
    "    \n",
    "    def sigmoid(self, a):\n",
    "        j = np.empty_like(a)\n",
    "        j[a < 0] = np.exp(a[a < 0]) / (1 + np.exp(a[a < 0]))\n",
    "        j[a > 0] = 1 / (1 + np.exp(-a[a > 0]))\n",
    "        \n",
    "        return j\n",
    "    \n",
    "    def forward_prop(self, X, Y):\n",
    "        x = X.T\n",
    "        y = Y.T\n",
    "        \n",
    "                \n",
    "        self.z1 = self.w1 @ x + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        \n",
    "        self.z2 = self.w2 @ self.a1 + self.b2\n",
    "        self.a2 = self.relu(self.z2)\n",
    "        \n",
    "        self.z3 = self.w3 @ self.a2 + self.b3\n",
    "        self.a3 = self.sigmoid(self.z3)\n",
    "        \n",
    "        \n",
    "        p = np.exp(self.a3)\n",
    "        self.p = p / p.sum(axis = 0, keepdims = True)\n",
    "        self.loss = np.mean(-y * np.log(self.p))\n",
    "    \n",
    "    def backward_prop(self, X, Y):\n",
    "        \n",
    "        dJ_dw1, dJ_db1, dJ_dw2, dJ_db2, dJ_dw3, dJ_db3 = self.compute_gradients(X, Y)\n",
    "        \n",
    "        # Backpropagation step\n",
    "        self.w3 -= self.learning_rate * dJ_dw3\n",
    "        self.b3 -= self.learning_rate * dJ_db3\n",
    "        \n",
    "        self.w2 -= self.learning_rate * dJ_dw2\n",
    "        self.b2 -= self.learning_rate * dJ_db2\n",
    "        \n",
    "        self.w1 -= self.learning_rate * dJ_dw1\n",
    "        self.b1 -= self.learning_rate * dJ_db1\n",
    "        \n",
    "    def compute_gradients(self, X, Y):\n",
    "        x = X.T\n",
    "        y = Y.T\n",
    "        \n",
    "        # Backward propagation\n",
    "#         print(self.p.shape, y.shape, self.a3.shape)\n",
    "        _, batch_size = self.p.shape\n",
    "        dJ_dz3 = (self.p - y) * self.a3 * (1 - self.a3)\n",
    "        dJ_dw3 = dJ_dz3 @ self.a2.T / batch_size\n",
    "        dJ_db3 = np.mean(dJ_dz3, axis = 1, keepdims = True)\n",
    "        \n",
    "        dJ_dz2 = (self.w3.T @ dJ_dz3) * self.relu_der(self.z2)\n",
    "        dJ_dw2 = dJ_dz2 @ self.a1.T / batch_size\n",
    "        dJ_db2 = np.mean(dJ_dz2, axis = 1, keepdims = True)\n",
    "        \n",
    "        dJ_dz1 = (self.w2.T @ dJ_dz2) * self.relu_der(self.z1)\n",
    "        dJ_dw1 = dJ_dz1 @ x.T / batch_size\n",
    "        dJ_db1 = np.mean(dJ_dz1, axis = 1, keepdims = True)\n",
    "        \n",
    "        return [dJ_dw1, dJ_db1, dJ_dw2, dJ_db2, dJ_dw3, dJ_db3]\n",
    "        \n",
    "        \n",
    "        p = list(net.parameters())\n",
    "        \n",
    "        print(\"Difference in gradient computation : \")\n",
    "        \n",
    "        print(\"Layer 3:\")\n",
    "        print(dJ_dw3.shape)\n",
    "        \n",
    "        print(np.linalg.norm(dJ_dw3 - p[-2].grad.detach().numpy()))\n",
    "        print(np.linalg.norm(dJ_db3 - p[-1].grad.detach().numpy()))\n",
    "        \n",
    "        print(\"Layer 2:\")\n",
    "        print(np.linalg.norm(dJ_dw2 - p[-4].grad.detach().numpy()))\n",
    "        print(np.linalg.norm(dJ_db2 - p[-3].grad.detach().numpy()))\n",
    "        \n",
    "        print(\"Layer 1:\")\n",
    "        print(np.linalg.norm(dJ_dw1 - p[-6].grad.detach().numpy()))\n",
    "        print(np.linalg.norm(dJ_db1 - p[-5].grad.detach().numpy()))\n",
    "        \n",
    "        \n",
    "    \n",
    "    def train(self, X, y, batch_size = 512, epochs = 2):\n",
    "        N = X.shape[0]\n",
    "        losses = []\n",
    "        loss = 0\n",
    "        for epoch in range(epochs):\n",
    "            for ind in range(0, N, batch_size):\n",
    "                self.forward_prop(X[ind:ind + batch_size], y[ind:ind + batch_size])\n",
    "                self.backward_prop(X[ind:ind + batch_size], y[ind:ind + batch_size])\n",
    "            self.forward_prop(X, y)\n",
    "            \n",
    "            print(\"Epoch {}: Loss {}\".format(epoch, self.loss))\n",
    "            losses.append(self.loss)\n",
    "        return losses\n",
    "        \n",
    "k = ModifiedMLP()\n",
    "# batch = 128\n",
    "k.forward_prop(X_train[:batch], y_t[:batch])\n",
    "    \n",
    "# k.compute_gradients(X_train[:batch], y_t[:batch])\n",
    "# k.backward_prop(X_train[:batch], y_t[:batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1c03d7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.2269845671344183\n",
      "Epoch 1: Loss 0.22315153966197462\n",
      "Epoch 2: Loss 0.2192314348852415\n",
      "Epoch 3: Loss 0.21520774925705313\n",
      "Epoch 4: Loss 0.21099740707817727\n",
      "Epoch 5: Loss 0.20667621225295071\n",
      "Epoch 6: Loss 0.20243686035628236\n",
      "Epoch 7: Loss 0.1984521832290408\n",
      "Epoch 8: Loss 0.19481867135507672\n",
      "Epoch 9: Loss 0.1915555606929262\n",
      "Epoch 10: Loss 0.1887843275942298\n",
      "Epoch 11: Loss 0.1864194991330541\n",
      "Epoch 12: Loss 0.1843873537781286\n",
      "Epoch 13: Loss 0.18263453179182476\n",
      "Epoch 14: Loss 0.1811110458082343\n",
      "Epoch 15: Loss 0.17977924983634366\n",
      "Epoch 16: Loss 0.17860165264667863\n",
      "Epoch 17: Loss 0.17754989264914675\n",
      "Epoch 18: Loss 0.17660326440286617\n",
      "Epoch 19: Loss 0.17574450776923536\n",
      "CPU times: user 3min 30s, sys: 29 s, total: 3min 59s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "losses = k.train(X_train, y_t, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0d4a2480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fe18a433d10>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(losses)), losses)\n",
    "plt.scatter(np.arange(len(losses)), losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e42ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
