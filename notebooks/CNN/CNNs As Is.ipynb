{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ad2667aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b2a40cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 3, 2])\n",
      "torch.Size([5])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "C_out = 5\n",
    "C_in = 4\n",
    "K = (3, 2)\n",
    "Kx, Ky = K\n",
    "stride = 2\n",
    "\n",
    "W = 35\n",
    "H = 32\n",
    "\n",
    "conv_layer = nn.Conv2d(in_channels = C_in, out_channels = C_out, kernel_size = K, stride = stride)\n",
    "\n",
    "print(conv_layer.weight.data.shape)\n",
    "print(conv_layer.bias.data.shape)\n",
    "\n",
    "weight = torch.randn(C_out, C_in, Kx, Ky)\n",
    "\n",
    "bias = torch.randn(C_out)\n",
    "print(weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "029aea24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([14, 4, 32, 35])\n",
      "\n",
      "Convolution weights:\n",
      " torch.Size([5, 4, 3, 2])\n",
      "Convolution bias: torch.Size([5])\n",
      "\n",
      "Output shape: torch.Size([14, 5, 15, 17])\n"
     ]
    }
   ],
   "source": [
    "N = 14\n",
    "\n",
    "\n",
    "# Create a dummy input tensor (1 channel, 4x4 image)\n",
    "input_tensor = torch.randn(N, C_in, H, W, requires_grad = True)\n",
    "\n",
    "# Reshape the input tensor to (batch_size, channels, height, width)\n",
    "input_tensor = input_tensor.reshape(N, C_in, H, W)\n",
    "input_tensor.retain_grad()\n",
    "\n",
    "# Define a convolutional layer\n",
    "\n",
    "\n",
    "# Set custom weights and bias for demonstration\n",
    "conv_layer.weight.data = weight\n",
    "conv_layer.bias.data = bias\n",
    "\n",
    "# Apply the convolutional layer to the input\n",
    "output = conv_layer(input_tensor)\n",
    "output.retain_grad()\n",
    "\n",
    "print(\"Input shape:\", input_tensor.shape)\n",
    "# print(\"Input:\\n\", input_tensor.squeeze())\n",
    "print(\"\\nConvolution weights:\\n\", conv_layer.weight.data.shape)\n",
    "print(\"Convolution bias:\", conv_layer.bias.data.shape)\n",
    "print(\"\\nOutput shape:\", output.shape)\n",
    "# print(\"Output:\\n\", output.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "23a50681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 17\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "outputHeight = math.floor((H - (Kx - 1) - 1) / stride + 1)\n",
    "outputWidth = math.floor((W - (Ky - 1) - 1) / stride + 1)\n",
    "print(outputHeight, outputWidth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942a4282",
   "metadata": {},
   "source": [
    "### Defining the convolution forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "53494b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correlate(A, B):\n",
    "    m, n = A.shape\n",
    "    p, q = B.shape\n",
    "    outputH = int((m - p) / stride) + 1\n",
    "    outputW = int((n - q) / stride) + 1\n",
    "    output = torch.zeros((outputH, outputW))\n",
    "    for i in range(outputH):\n",
    "        for j in range(outputW):\n",
    "            value = 0\n",
    "            for u in range(p):\n",
    "                for v in range(q):\n",
    "                    value += A[stride * i + u, stride * j + v] * B[u, v]\n",
    "            output[i, j] = value\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4b6e3799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 65.,  85., 105.],\n",
       "        [205., 225., 245.],\n",
       "        [345., 365., 385.]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stride = 2\n",
    "A = torch.Tensor(np.arange(1, 50).reshape(7,7))\n",
    "B = torch.Tensor(np.arange(1, 5).reshape(2, 2))\n",
    "\n",
    "cross_correlate(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4d382028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(): \n",
    "    output_tensor = torch.zeros((N, C_out, outputHeight, outputWidth))\n",
    "    for i in range(N):\n",
    "        for j in range(C_out):\n",
    "            for k in range(C_in):\n",
    "                output_tensor[i, j] += cross_correlate(input_tensor[i, k], weight[j, k])\n",
    "            output_tensor[i, j] += bias[j]\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "41d42b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor = convolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c129e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = output.sum()\n",
    "l.backward()\n",
    "output_grad = output.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "771dea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(torch.norm(output_tensor - output) < 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7b500f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correlation_grad(A, B, C_grad):\n",
    "    p, q = B.shape\n",
    "    m, n = A.shape\n",
    "    weight_grad = torch.zeros(p, q)\n",
    "    \n",
    "    xLim = math.floor((m - p) / stride + 1)\n",
    "    yLim = math.floor((n - q) / stride + 1)\n",
    "    for a in range(p):\n",
    "        for b in range(q):\n",
    "            value = 0\n",
    "            for i in range(xLim):\n",
    "                for j in range(yLim):\n",
    "                    value += C_grad[i, j].item() * A[i * stride + a, j * stride + b].item()\n",
    "            weight_grad[a, b] = value\n",
    "    return weight_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "69bbb920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_grad():\n",
    "    weight_grad = torch.zeros(C_out, C_in, Kx, Ky)\n",
    "    for j in range(C_out):\n",
    "        for k in range(C_in):\n",
    "            w_grad = torch.zeros(Kx, Ky)\n",
    "            for i in range(N):\n",
    "                w_grad += cross_correlation_grad(input_tensor[i, k], weight[j, k], output_grad[i, j])\n",
    "            weight_grad[j, k] += w_grad\n",
    "    return weight_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6fc35c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_grad = convolution_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e40af7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 4, 32, 35])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2474cf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 3, 2])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6fd37867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 3, 2])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5d52c530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  18.4215,   83.8345],\n",
       "          [ -51.6044,   32.2838],\n",
       "          [  14.3864,   16.1129]],\n",
       "\n",
       "         [[  14.4165,  -92.5791],\n",
       "          [-168.0305,  102.9635],\n",
       "          [  31.7107, -159.5172]],\n",
       "\n",
       "         [[ 201.1115,  -69.9786],\n",
       "          [  47.7925,  136.3253],\n",
       "          [ 186.3972,  -46.2984]],\n",
       "\n",
       "         [[ -50.9993,   16.0457],\n",
       "          [-224.6265,  -94.3197],\n",
       "          [ -11.9449,  -16.3227]]],\n",
       "\n",
       "\n",
       "        [[[  18.4215,   83.8345],\n",
       "          [ -51.6044,   32.2838],\n",
       "          [  14.3864,   16.1129]],\n",
       "\n",
       "         [[  14.4165,  -92.5791],\n",
       "          [-168.0305,  102.9635],\n",
       "          [  31.7107, -159.5172]],\n",
       "\n",
       "         [[ 201.1115,  -69.9786],\n",
       "          [  47.7925,  136.3253],\n",
       "          [ 186.3972,  -46.2984]],\n",
       "\n",
       "         [[ -50.9993,   16.0457],\n",
       "          [-224.6265,  -94.3197],\n",
       "          [ -11.9449,  -16.3227]]],\n",
       "\n",
       "\n",
       "        [[[  18.4215,   83.8345],\n",
       "          [ -51.6044,   32.2838],\n",
       "          [  14.3864,   16.1129]],\n",
       "\n",
       "         [[  14.4165,  -92.5791],\n",
       "          [-168.0305,  102.9635],\n",
       "          [  31.7107, -159.5172]],\n",
       "\n",
       "         [[ 201.1115,  -69.9786],\n",
       "          [  47.7925,  136.3253],\n",
       "          [ 186.3972,  -46.2984]],\n",
       "\n",
       "         [[ -50.9993,   16.0457],\n",
       "          [-224.6265,  -94.3197],\n",
       "          [ -11.9449,  -16.3227]]],\n",
       "\n",
       "\n",
       "        [[[  18.4215,   83.8345],\n",
       "          [ -51.6044,   32.2838],\n",
       "          [  14.3864,   16.1129]],\n",
       "\n",
       "         [[  14.4165,  -92.5791],\n",
       "          [-168.0305,  102.9635],\n",
       "          [  31.7107, -159.5172]],\n",
       "\n",
       "         [[ 201.1115,  -69.9786],\n",
       "          [  47.7925,  136.3253],\n",
       "          [ 186.3972,  -46.2984]],\n",
       "\n",
       "         [[ -50.9993,   16.0457],\n",
       "          [-224.6265,  -94.3197],\n",
       "          [ -11.9449,  -16.3227]]],\n",
       "\n",
       "\n",
       "        [[[  18.4215,   83.8345],\n",
       "          [ -51.6044,   32.2838],\n",
       "          [  14.3864,   16.1129]],\n",
       "\n",
       "         [[  14.4165,  -92.5791],\n",
       "          [-168.0305,  102.9635],\n",
       "          [  31.7107, -159.5172]],\n",
       "\n",
       "         [[ 201.1115,  -69.9786],\n",
       "          [  47.7925,  136.3253],\n",
       "          [ 186.3972,  -46.2984]],\n",
       "\n",
       "         [[ -50.9993,   16.0457],\n",
       "          [-224.6265,  -94.3197],\n",
       "          [ -11.9449,  -16.3227]]]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5947ef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "28558ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input tensor A of shape (N,N) where N=5\n",
    "A = torch.tensor([\n",
    "    [5, 27, 9, -15, 12],\n",
    "    [2, 4, -2, -7, 8],\n",
    "    [-8, 16, 5, 23, -6],\n",
    "    [-17, 7, 9, -14, 3],\n",
    "    [10, -5, 13, 6, -11]\n",
    "], dtype=torch.float32).requires_grad_()\n",
    "\n",
    "# Define the kernel tensor B of shape (K,K) where K=2\n",
    "B = torch.tensor([\n",
    "    [8, 4],\n",
    "    [2, 6]\n",
    "], dtype=torch.float32).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bc1bb517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 5, 5])\n",
      "Kernel shape: torch.Size([1, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Reshape input tensor A to match conv2d input format (N, C, H, W)\n",
    "# N = batch size (1)\n",
    "# C = input channels (1)\n",
    "# H = height (4)\n",
    "# W = width (4)\n",
    "A = A.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "print(\"Input shape:\", A.shape)  # Should be (1, 1, 4, 4)\n",
    "A.retain_grad()\n",
    "# Reshape kernel tensor B to match conv2d weight format \n",
    "# (out_channels, in_channels, kernel_height, kernel_width)\n",
    "B = B.unsqueeze(0).unsqueeze(0)  # Add out_channels and in_channels dimensions\n",
    "B.retain_grad()\n",
    "print(\"Kernel shape:\", B.shape)  # Should be (1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "11640be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Conv2d layer\n",
    "# in_channels=1: we have a single input channel\n",
    "# out_channels=1: we want a single output channel\n",
    "# kernel_size=1: our kernel is 1x1\n",
    "# stride=1: move the kernel by 1 pixel at a time\n",
    "# padding=0: no padding\n",
    "conv_layer = nn.Conv2d(in_channels=1, \n",
    "                      out_channels=1,\n",
    "                      kernel_size=1,\n",
    "                      stride=stride,\n",
    "                      padding=0,\n",
    "                      bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d8648a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output shape: torch.Size([1, 1, 2, 2])\n",
      "\n",
      "Convolution output:\n",
      "[[176. -34.]\n",
      " [  8.  66.]]\n"
     ]
    }
   ],
   "source": [
    "# Set the weight of conv_layer to our kernel B\n",
    "conv_layer.weight = nn.Parameter(B)\n",
    "\n",
    "# Perform convolution\n",
    "output = conv_layer(A)\n",
    "# output.requires_grad_()\n",
    "output.retain_grad()\n",
    "\n",
    "print(\"\\nOutput shape:\", output.shape)  # Should be (1, 1, 4, 4)\n",
    "print(\"\\nConvolution output:\")\n",
    "print(output.squeeze().detach().numpy())  # Remove batch and channel dimensions for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "59a4d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = output.sum()\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "766bda4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[8., 4., 8., 4., 0.],\n",
      "          [2., 6., 2., 6., 0.],\n",
      "          [8., 4., 8., 4., 0.],\n",
      "          [2., 6., 2., 6., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "print(A.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cf794966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 11.,  51.],\n",
       "          [ -8., -10.]]]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b88f0b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1.],\n",
       "          [1., 1.]]]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bbdbe34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A.squeeze()\n",
    "B = conv_layer.weight.data.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a70e2212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[176., -34.],\n",
       "          [  8.,  66.]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f149ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = output.squeeze()\n",
    "C_grad = output.grad.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b84842f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[176., -34.],\n",
       "        [  8.,  66.]], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "24fdbb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "92730934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 11.,  51.],\n",
       "        [ -8., -10.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_correlation_grad(A, B, C_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824cdb35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
