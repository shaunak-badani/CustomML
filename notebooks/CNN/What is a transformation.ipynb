{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e04bf9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e380bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (255 * np.random.random((32, 32, 3))).round().astype('uint8') # generate some random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f008f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL_image = Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20fb17dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIL_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09e07ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL_image_alt_init = Image.new('RGB', (32, 32), color = '#564354')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "50fecefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAALUlEQVR4nGMMcw5hoCVgoqnpoxaMWjBqwagFoxaMWjBqwagFoxaMWjBqARUBAMicAS3ZEJTeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIL_image_alt_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88356dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b0e7888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_img = transform(Image.fromarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "efb4095e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8353, 0.6000, 0.2039,  ..., 0.0667, 0.7098, 0.1294],\n",
       "         [0.5804, 0.9098, 0.3255,  ..., 0.1804, 0.7216, 0.0667],\n",
       "         [0.1451, 0.9412, 0.0078,  ..., 0.6510, 0.8980, 0.1176],\n",
       "         ...,\n",
       "         [0.8471, 0.0980, 0.3490,  ..., 0.0118, 0.9961, 0.4118],\n",
       "         [0.5176, 0.9176, 0.4392,  ..., 0.0902, 0.5059, 0.6431],\n",
       "         [0.9922, 0.0549, 0.4588,  ..., 0.5843, 0.6510, 0.1137]],\n",
       "\n",
       "        [[0.2588, 0.0000, 0.7373,  ..., 0.1725, 0.0941, 0.4863],\n",
       "         [0.0667, 0.0745, 0.2941,  ..., 0.1176, 0.4706, 0.6510],\n",
       "         [0.2863, 0.4745, 0.7804,  ..., 0.9922, 0.9020, 0.5804],\n",
       "         ...,\n",
       "         [0.0667, 0.3451, 0.4706,  ..., 0.2667, 0.0275, 0.5451],\n",
       "         [0.5647, 0.5961, 0.0000,  ..., 0.8314, 0.9216, 0.1529],\n",
       "         [0.4000, 0.6549, 0.9020,  ..., 0.4588, 0.8000, 0.1490]],\n",
       "\n",
       "        [[0.5412, 0.8000, 0.1098,  ..., 0.7412, 0.8196, 0.4196],\n",
       "         [0.1961, 0.6078, 0.1412,  ..., 0.1020, 0.5255, 0.6196],\n",
       "         [0.8118, 0.8902, 0.5294,  ..., 0.4353, 0.6392, 0.0078],\n",
       "         ...,\n",
       "         [0.8667, 0.8353, 0.3686,  ..., 0.9882, 0.8000, 0.5804],\n",
       "         [0.6980, 0.2118, 0.0353,  ..., 0.6000, 0.5608, 0.1216],\n",
       "         [0.7922, 0.2706, 0.4510,  ..., 0.3333, 0.2510, 0.5608]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712b65d",
   "metadata": {},
   "source": [
    "## Checking to see if the following formula is correct : \n",
    "\n",
    " transpose((256 * transform(image).numpy()), (1, 2, 0) == PIL_image.numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f57e6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val1 = np.floor(np.transpose((256 * tensor_img).numpy(), (1, 2, 0)))\n",
    "val2 = np.asarray(PIL_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e50097b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(val1 - val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bcc0ec0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b0680a",
   "metadata": {},
   "source": [
    "ToTensor does the following : \n",
    "* changes the image from (3, 32, 32) to (32, 32, 3)\n",
    "* Divides values by 256, and floors to nearest integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a9cc76",
   "metadata": {},
   "source": [
    "### From docs \n",
    "> Convert a PIL Image or ndarray to tensor and scale the values accordingly. <br />\n",
    "This transform does not support torchscript. <br /> Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8. <br />In the other cases, tensors are returned without scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d01b8",
   "metadata": {},
   "source": [
    "# What is normalize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a71ee787",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_normalize = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f652a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_image = transform_normalize(tensor_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ad3e9651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c82e3fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "43af4e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.5, 0.5, 0.5)\n",
    "std = (0.5, 0.5, 0.5)\n",
    "tensor_mean = np.array(mean).reshape((-1, 1, 1))\n",
    "tensor_std = np.array(std).reshape((-1, 1, 1))\n",
    "transformed_tensor = (tensor_img - tensor_mean) / tensor_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6124db9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.343838238799516e-07"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(transformed_tensor - normalized_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
